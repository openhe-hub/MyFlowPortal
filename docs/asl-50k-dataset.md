# ASL 50K Dataset

## Overview

ASL 50K is a sentence-level American Sign Language video dataset. Each sentence video is synthesized by concatenating word-level sign videos from SignASL_New. The dataset includes 20,000 sentence videos with English text annotations.

## Data Sources

| Source | Path | Description |
|--------|------|-------------|
| Sentences CSV | `/home/nyuair/118-data-003/spamo/asl_50k/dict/sentences.csv` | 50K sentence definitions (first 20K used) |
| Word Videos | `/home/nyuair/118-data-003/SignASL_New/` | ~48,845 word-level MP4 clips |

## Generated Datasets

### `datasets/asl_50k/` — Full-frame sentence videos

- **Count**: 20,000 MP4 files
- **Size**: ~33 GB
- **Format**: H264, mixed resolution (mostly 640x354@29.97fps, some 320x180@24fps)
- **Naming**: `{sentence_id}.mp4` (e.g., `W10_000001.mp4`)
- **Avg frames**: ~2,094 per video

Generated by concatenating word-level videos using ffmpeg concat demuxer with stream copy (no re-encoding).

### `datasets/asl_50k_10x/` — 10x temporally downsampled

- **Count**: 20,000 MP4 files
- **Size**: ~16 GB
- **Format**: MPEG4, uniform 640x360@30fps
- **Naming**: `{sentence_id}.mp4` (same as above)
- **Avg frames**: ~210 per video

Every 10th frame is kept from the full-frame version, with all frames resized to 640x360.

### Annotations

`datasets/asl_50k_10x/sentences.csv`:

```csv
video_name,sentence
W10_000001.mp4,I was surprised when my manager suggested we all work remotely tomorrow.
```

## Pipeline Scripts

### `scripts/concat_asl50k_sentences.py`

Concatenates word-level videos into sentence-level videos.

```bash
python scripts/concat_asl50k_sentences.py                          # full 20K run
python scripts/concat_asl50k_sentences.py --max-rows 100 --workers 8  # test run
```

- Uses `/usr/bin/ffmpeg` (system ffmpeg with libx264; conda's lacks it)
- Concat demuxer + stream copy — no re-encoding, ~17.6 sent/s
- Skips `NO_REF` word entries (missing ASL signs)
- Supports resume (skips existing output files)

### `scripts/subsample_asl50k.py`

10x temporal downsampling using OpenCV.

```bash
python scripts/subsample_asl50k.py                        # full 20K run
python scripts/subsample_asl50k.py --factor 5 --workers 16  # custom
```

- Uses `grab()`/`read()` to skip decoding unused frames
- Resizes all frames to uniform 640x360
- Supports resume

## Notes

- The full-frame videos (`asl_50k/`) have incorrect container duration metadata due to mixed timebases from concat. Frame counts and actual playback are correct. OpenCV reads all frames reliably.
- About 4.3% of sentences have partial word coverage (NO_REF entries skipped), averaging ~1 missing word per affected sentence.
